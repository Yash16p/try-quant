{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using single threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from bayes_opt import BayesianOptimization\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, parse_dates=['datetime'], index_col='datetime')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering (add interaction features for open, high, low, close)\n",
    "def feature_engineering(df):\n",
    "    # Interaction features for open, high, low, close\n",
    "    df['high_low_spread'] = df['high'] - df['low']\n",
    "    df['open_close_diff'] = df['close'] - df['open']\n",
    "    \n",
    "    # Existing features\n",
    "    df['volume_SMA_20'] = df['volume'].rolling(20).mean()  # Needed for Volume_ratio\n",
    "    df['RSI_14'] = ta.rsi(df['close'], length=14)\n",
    "    df['macd'] = ta.macd(df['close'], fast=12, slow=26, signal=9)['MACD_12_26_9']\n",
    "    df['MACD_signal'] = ta.macd(df['close'], fast=12, slow=26, signal=9)['MACDs_12_26_9']\n",
    "    df['ATR_14'] = ta.atr(df['high'], df['low'], df['close'], length=14)\n",
    "    df['OBV'] = ta.obv(df['close'], df['volume'])\n",
    "    df['ROC_10'] = ta.roc(df['close'], length=10)\n",
    "    df['price_delta'] = df['close'].diff()\n",
    "    df['volatility'] = df['close'].rolling(20).std()\n",
    "    df['RSI_lag_1'] = df['RSI_14'].shift(1)\n",
    "    df['hour'] = df.index.hour\n",
    "    df['dayofweek'] = df.index.dayofweek\n",
    "    df['ATR_ratio'] = df['ATR_14'] / df['close']\n",
    "    df['Volume_ratio'] = df['volume'] / df['volume_SMA_20']\n",
    "    df['trend'] = df['close'].rolling(20).mean() / df['close']\n",
    "    df['price_vol_ratio'] = np.where(df['volume'] == 0, 0, df['close'] / df['volume'])\n",
    "    df['momentum'] = df['close'].diff(5)  # Needed for mom_div\n",
    "    df['mom_div'] = df['momentum'] - df['momentum'].shift(5)\n",
    "    \n",
    "    # Drop unnecessary columns, but keep open, high, low, close\n",
    "    df.drop(columns=['momentum', 'volume_SMA_20', \n",
    "                     'bollinger_high', 'bollinger_low', 'bollinger_mavg', 'minute', 'close_lag_1'], \n",
    "            inplace=True, errors='ignore')\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target\n",
    "def define_target(df):\n",
    "    df['target'] = np.where(df['close'].shift(-1) > df['close'] * 1.005, 1, 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample majority class\n",
    "def undersample_majority(X, y, target_size=60000):\n",
    "    rus = RandomUnderSampler(sampling_strategy={0: target_size, 1: np.sum(y == 1)}, random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize XGBoost\n",
    "def optimize_xgboost(X_train, y_train, X_val, y_val):\n",
    "    def xgb_eval(max_depth, learning_rate, colsample_bytree, subsample, gamma, min_child_weight, num_boost_round):\n",
    "        params = {\n",
    "            'max_depth': int(max_depth),\n",
    "            'learning_rate': learning_rate,\n",
    "            'colsample_bytree': colsample_bytree,\n",
    "            'subsample': subsample,\n",
    "            'gamma': gamma,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'eval_metric': 'logloss',\n",
    "            'scale_pos_weight': 4.0,  # Adjusted\n",
    "            'objective': 'binary:logistic'\n",
    "        }\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        model = xgb.train(params, dtrain, num_boost_round=int(num_boost_round), evals=[(dval, 'val')], \n",
    "                          early_stopping_rounds=10, verbose_eval=False)\n",
    "        return -model.best_score\n",
    "    \n",
    "    param_bounds = {\n",
    "        'max_depth': (3, 8),\n",
    "        'learning_rate': (0.05, 0.5),\n",
    "        'colsample_bytree': (0.6, 1.0),\n",
    "        'subsample': (0.6, 1.0),\n",
    "        'gamma': (0, 5),\n",
    "        'min_child_weight': (5, 10),\n",
    "        'num_boost_round': (50, 500)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(f=xgb_eval, pbounds=param_bounds, random_state=42)\n",
    "    optimizer.maximize(n_iter=30)\n",
    "    \n",
    "    best_params = optimizer.max['params']\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['num_boost_round'] = int(best_params['num_boost_round'])\n",
    "    best_params['min_child_weight'] = int(best_params['min_child_weight'])\n",
    "    best_params['eval_metric'] = 'logloss'\n",
    "    best_params['scale_pos_weight'] = 4.0\n",
    "    best_params['objective'] = 'binary:logistic'\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_model(X_train, y_train, X_val, y_val, best_params):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    params = {k: v for k, v in best_params.items() if k != 'num_boost_round'}\n",
    "    print(\"Training with parameters:\", params)\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=best_params['num_boost_round'],\n",
    "        evals=[(dval, 'val')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=True\n",
    "    )\n",
    "    joblib.dump(model, \"optimized_xgboost.joblib\")\n",
    "    print(\"Model saved successfully.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with new threshold\n",
    "def evaluate_model(model, best_params, X_train, y_train, X_val, y_val, X_test, y_test, feature_cols):\n",
    "    params = {k: v for k, v in best_params.items() if k not in ['eval_metric', 'num_boost_round', 'objective']}\n",
    "    xgb_clf = XGBClassifier(**params, n_estimators=model.best_iteration + 1)\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_probs_raw = np.clip(model.predict(dtest), 0, 1)\n",
    "    \n",
    "    print(\"Raw probabilities (sample):\", y_probs_raw[:10])\n",
    "    \n",
    "    lr = LogisticRegression(class_weight={0: 1, 1: 1}, penalty='l1', solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_probs_lr = lr.predict_proba(X_test)[:, 1]\n",
    "    y_probs_ensemble = 0.6 * y_probs_raw + 0.4 * y_probs_lr\n",
    "    \n",
    "    threshold = 0.6  # Increased to 0.6\n",
    "    y_pred = (y_probs_ensemble > threshold).astype(int)\n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Print feature importances as text\n",
    "    importance = model.get_score(importance_type='weight')\n",
    "    print(\"\\nFeature Importances:\")\n",
    "    for i, col in enumerate(feature_cols):\n",
    "        score = importance.get(f\"f{i}\", 0)\n",
    "        print(f\"{col} (f{i}): {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns: ['open', 'high', 'low', 'close', 'volume', 'SMA_20', 'RSI_14', 'MACD', 'ATR_14', 'OBV', 'ROC_10', 'price_delta', 'volatility', 'RSI_lag_1', 'hour', 'dayofweek', 'ATR_ratio', 'Volume_ratio', 'MACD_signal', 'high_low_spread', 'open_close_diff', 'macd', 'trend', 'price_vol_ratio', 'mom_div']\n",
      "Original class distribution: [83371 10459]\n",
      "Class distribution after undersampling: [60000 10459]\n",
      "Train (before SMOTE) class distribution: [36044  6231]\n",
      "Validation class distribution: [11959  2133]\n",
      "Test class distribution: [11997  2095]\n",
      "Train class distribution (after SMOTE): [36044 30000]\n",
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | num_bo... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-0.5766  \u001b[39m | \u001b[39m0.7498   \u001b[39m | \u001b[39m4.754    \u001b[39m | \u001b[39m0.3794   \u001b[39m | \u001b[39m5.993    \u001b[39m | \u001b[39m5.78     \u001b[39m | \u001b[39m120.2    \u001b[39m | \u001b[39m0.6232   \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-0.5857  \u001b[39m | \u001b[39m0.9465   \u001b[39m | \u001b[39m3.006    \u001b[39m | \u001b[39m0.3686   \u001b[39m | \u001b[39m3.103    \u001b[39m | \u001b[39m9.85     \u001b[39m | \u001b[39m424.6    \u001b[39m | \u001b[39m0.6849   \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m-0.5715  \u001b[39m | \u001b[35m0.6727   \u001b[39m | \u001b[35m0.917    \u001b[39m | \u001b[35m0.1869   \u001b[39m | \u001b[35m5.624    \u001b[39m | \u001b[35m7.16     \u001b[39m | \u001b[35m181.1    \u001b[39m | \u001b[35m0.8447   \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-0.5845  \u001b[39m | \u001b[39m0.6558   \u001b[39m | \u001b[39m1.461    \u001b[39m | \u001b[39m0.2149   \u001b[39m | \u001b[39m5.28     \u001b[39m | \u001b[39m8.926    \u001b[39m | \u001b[39m139.9    \u001b[39m | \u001b[39m0.8057   \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-0.5729  \u001b[39m | \u001b[39m0.837    \u001b[39m | \u001b[39m0.2323   \u001b[39m | \u001b[39m0.3234   \u001b[39m | \u001b[39m3.853    \u001b[39m | \u001b[39m5.325    \u001b[39m | \u001b[39m477.0    \u001b[39m | \u001b[39m0.9863   \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m-0.5478  \u001b[39m | \u001b[35m0.6227   \u001b[39m | \u001b[35m1.616    \u001b[39m | \u001b[35m0.183    \u001b[39m | \u001b[35m6.585    \u001b[39m | \u001b[35m7.373    \u001b[39m | \u001b[35m181.1    \u001b[39m | \u001b[35m0.8282   \u001b[39m |\n",
      "| \u001b[35m7        \u001b[39m | \u001b[35m-0.5367  \u001b[39m | \u001b[35m0.961    \u001b[39m | \u001b[35m2.253    \u001b[39m | \u001b[35m0.3141   \u001b[39m | \u001b[35m6.649    \u001b[39m | \u001b[35m6.995    \u001b[39m | \u001b[35m181.5    \u001b[39m | \u001b[35m0.6356   \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-0.5371  \u001b[39m | \u001b[39m0.8388   \u001b[39m | \u001b[39m2.411    \u001b[39m | \u001b[39m0.4287   \u001b[39m | \u001b[39m7.6      \u001b[39m | \u001b[39m7.583    \u001b[39m | \u001b[39m182.3    \u001b[39m | \u001b[39m0.674    \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m-0.56    \u001b[39m | \u001b[39m0.6763   \u001b[39m | \u001b[39m3.697    \u001b[39m | \u001b[39m0.2161   \u001b[39m | \u001b[39m6.512    \u001b[39m | \u001b[39m7.102    \u001b[39m | \u001b[39m181.4    \u001b[39m | \u001b[39m0.9565   \u001b[39m |\n",
      "| \u001b[39m10       \u001b[39m | \u001b[39m-0.5401  \u001b[39m | \u001b[39m0.9876   \u001b[39m | \u001b[39m1.188    \u001b[39m | \u001b[39m0.1277   \u001b[39m | \u001b[39m7.013    \u001b[39m | \u001b[39m6.092    \u001b[39m | \u001b[39m182.5    \u001b[39m | \u001b[39m0.6012   \u001b[39m |\n",
      "| \u001b[35m11       \u001b[39m | \u001b[35m-0.509   \u001b[39m | \u001b[35m0.7387   \u001b[39m | \u001b[35m1.29     \u001b[39m | \u001b[35m0.2512   \u001b[39m | \u001b[35m7.481    \u001b[39m | \u001b[35m7.51     \u001b[39m | \u001b[35m184.6    \u001b[39m | \u001b[35m0.7565   \u001b[39m |\n",
      "| \u001b[39m12       \u001b[39m | \u001b[39m-0.516   \u001b[39m | \u001b[39m0.9513   \u001b[39m | \u001b[39m2.116    \u001b[39m | \u001b[39m0.2414   \u001b[39m | \u001b[39m7.81     \u001b[39m | \u001b[39m6.869    \u001b[39m | \u001b[39m185.7    \u001b[39m | \u001b[39m0.674    \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-0.5311  \u001b[39m | \u001b[39m0.9806   \u001b[39m | \u001b[39m0.4191   \u001b[39m | \u001b[39m0.2324   \u001b[39m | \u001b[39m6.013    \u001b[39m | \u001b[39m7.291    \u001b[39m | \u001b[39m186.3    \u001b[39m | \u001b[39m0.7843   \u001b[39m |\n",
      "| \u001b[39m14       \u001b[39m | \u001b[39m-0.5573  \u001b[39m | \u001b[39m0.7528   \u001b[39m | \u001b[39m2.79     \u001b[39m | \u001b[39m0.1379   \u001b[39m | \u001b[39m6.95     \u001b[39m | \u001b[39m8.832    \u001b[39m | \u001b[39m185.0    \u001b[39m | \u001b[39m0.9091   \u001b[39m |\n",
      "| \u001b[35m15       \u001b[39m | \u001b[35m-0.4894  \u001b[39m | \u001b[35m0.8342   \u001b[39m | \u001b[35m0.5968   \u001b[39m | \u001b[35m0.318    \u001b[39m | \u001b[35m7.601    \u001b[39m | \u001b[35m5.876    \u001b[39m | \u001b[35m185.9    \u001b[39m | \u001b[35m0.9779   \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-0.5279  \u001b[39m | \u001b[39m0.6199   \u001b[39m | \u001b[39m0.4832   \u001b[39m | \u001b[39m0.141    \u001b[39m | \u001b[39m7.798    \u001b[39m | \u001b[39m5.485    \u001b[39m | \u001b[39m184.7    \u001b[39m | \u001b[39m0.798    \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m-0.5219  \u001b[39m | \u001b[39m0.6089   \u001b[39m | \u001b[39m0.6152   \u001b[39m | \u001b[39m0.1423   \u001b[39m | \u001b[39m7.706    \u001b[39m | \u001b[39m5.083    \u001b[39m | \u001b[39m187.6    \u001b[39m | \u001b[39m0.9023   \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m-0.5051  \u001b[39m | \u001b[39m0.6982   \u001b[39m | \u001b[39m0.9436   \u001b[39m | \u001b[39m0.3651   \u001b[39m | \u001b[39m7.894    \u001b[39m | \u001b[39m7.739    \u001b[39m | \u001b[39m186.1    \u001b[39m | \u001b[39m0.9123   \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-0.5052  \u001b[39m | \u001b[39m0.6324   \u001b[39m | \u001b[39m0.258    \u001b[39m | \u001b[39m0.2217   \u001b[39m | \u001b[39m7.933    \u001b[39m | \u001b[39m7.384    \u001b[39m | \u001b[39m187.3    \u001b[39m | \u001b[39m0.8725   \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-0.5397  \u001b[39m | \u001b[39m0.7074   \u001b[39m | \u001b[39m1.336    \u001b[39m | \u001b[39m0.1055   \u001b[39m | \u001b[39m7.802    \u001b[39m | \u001b[39m9.997    \u001b[39m | \u001b[39m191.5    \u001b[39m | \u001b[39m0.9591   \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-0.5002  \u001b[39m | \u001b[39m0.9013   \u001b[39m | \u001b[39m0.09417  \u001b[39m | \u001b[39m0.232    \u001b[39m | \u001b[39m7.731    \u001b[39m | \u001b[39m9.147    \u001b[39m | \u001b[39m184.7    \u001b[39m | \u001b[39m0.9306   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-0.5535  \u001b[39m | \u001b[39m0.8554   \u001b[39m | \u001b[39m2.859    \u001b[39m | \u001b[39m0.1506   \u001b[39m | \u001b[39m5.321    \u001b[39m | \u001b[39m5.627    \u001b[39m | \u001b[39m292.8    \u001b[39m | \u001b[39m0.7831   \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-0.5653  \u001b[39m | \u001b[39m0.9276   \u001b[39m | \u001b[39m4.42     \u001b[39m | \u001b[39m0.3068   \u001b[39m | \u001b[39m4.256    \u001b[39m | \u001b[39m7.168    \u001b[39m | \u001b[39m362.9    \u001b[39m | \u001b[39m0.9255   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-0.5549  \u001b[39m | \u001b[39m0.8001   \u001b[39m | \u001b[39m0.3247   \u001b[39m | \u001b[39m0.09134  \u001b[39m | \u001b[39m7.166    \u001b[39m | \u001b[39m9.999    \u001b[39m | \u001b[39m187.8    \u001b[39m | \u001b[39m0.9251   \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-0.4993  \u001b[39m | \u001b[39m0.6841   \u001b[39m | \u001b[39m0.09955  \u001b[39m | \u001b[39m0.3685   \u001b[39m | \u001b[39m7.744    \u001b[39m | \u001b[39m7.09     \u001b[39m | \u001b[39m184.4    \u001b[39m | \u001b[39m0.9454   \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-0.5101  \u001b[39m | \u001b[39m0.6494   \u001b[39m | \u001b[39m2.112    \u001b[39m | \u001b[39m0.2182   \u001b[39m | \u001b[39m7.521    \u001b[39m | \u001b[39m7.587    \u001b[39m | \u001b[39m197.6    \u001b[39m | \u001b[39m0.9491   \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-0.5707  \u001b[39m | \u001b[39m0.8294   \u001b[39m | \u001b[39m4.785    \u001b[39m | \u001b[39m0.1661   \u001b[39m | \u001b[39m7.583    \u001b[39m | \u001b[39m7.755    \u001b[39m | \u001b[39m197.6    \u001b[39m | \u001b[39m0.984    \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-0.5043  \u001b[39m | \u001b[39m0.9138   \u001b[39m | \u001b[39m0.007028 \u001b[39m | \u001b[39m0.2513   \u001b[39m | \u001b[39m7.314    \u001b[39m | \u001b[39m7.39     \u001b[39m | \u001b[39m198.9    \u001b[39m | \u001b[39m0.7957   \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-0.567   \u001b[39m | \u001b[39m0.9478   \u001b[39m | \u001b[39m0.06669  \u001b[39m | \u001b[39m0.1844   \u001b[39m | \u001b[39m5.666    \u001b[39m | \u001b[39m7.173    \u001b[39m | \u001b[39m196.7    \u001b[39m | \u001b[39m0.8219   \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-0.5077  \u001b[39m | \u001b[39m0.9949   \u001b[39m | \u001b[39m1.409    \u001b[39m | \u001b[39m0.2003   \u001b[39m | \u001b[39m7.858    \u001b[39m | \u001b[39m6.191    \u001b[39m | \u001b[39m199.4    \u001b[39m | \u001b[39m0.6815   \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m-0.5214  \u001b[39m | \u001b[39m0.8642   \u001b[39m | \u001b[39m1.237    \u001b[39m | \u001b[39m0.4808   \u001b[39m | \u001b[39m7.325    \u001b[39m | \u001b[39m8.104    \u001b[39m | \u001b[39m200.5    \u001b[39m | \u001b[39m0.9159   \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m-0.5451  \u001b[39m | \u001b[39m0.9624   \u001b[39m | \u001b[39m0.5705   \u001b[39m | \u001b[39m0.1013   \u001b[39m | \u001b[39m7.914    \u001b[39m | \u001b[39m8.775    \u001b[39m | \u001b[39m197.2    \u001b[39m | \u001b[39m0.8044   \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m-0.5404  \u001b[39m | \u001b[39m0.986    \u001b[39m | \u001b[39m0.3468   \u001b[39m | \u001b[39m0.2661   \u001b[39m | \u001b[39m5.104    \u001b[39m | \u001b[39m5.75     \u001b[39m | \u001b[39m200.6    \u001b[39m | \u001b[39m0.9596   \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m-0.5579  \u001b[39m | \u001b[39m0.6717   \u001b[39m | \u001b[39m1.683    \u001b[39m | \u001b[39m0.4565   \u001b[39m | \u001b[39m7.666    \u001b[39m | \u001b[39m5.929    \u001b[39m | \u001b[39m198.0    \u001b[39m | \u001b[39m0.7074   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m-0.5276  \u001b[39m | \u001b[39m0.9122   \u001b[39m | \u001b[39m0.2099   \u001b[39m | \u001b[39m0.3433   \u001b[39m | \u001b[39m7.918    \u001b[39m | \u001b[39m6.941    \u001b[39m | \u001b[39m199.6    \u001b[39m | \u001b[39m0.6838   \u001b[39m |\n",
      "=============================================================================================================\n",
      "Training with parameters: {'colsample_bytree': 0.8342313146487481, 'gamma': 0.5967870711220841, 'learning_rate': 0.3179747889634786, 'max_depth': 7, 'min_child_weight': 5, 'subsample': 0.9778861724541164, 'eval_metric': 'logloss', 'scale_pos_weight': 4.0, 'objective': 'binary:logistic'}\n",
      "[0]\tval-logloss:1.06011\n",
      "[1]\tval-logloss:1.00485\n",
      "[2]\tval-logloss:0.97655\n",
      "[3]\tval-logloss:0.94012\n",
      "[4]\tval-logloss:0.92015\n",
      "[5]\tval-logloss:0.90982\n",
      "[6]\tval-logloss:0.89896\n",
      "[7]\tval-logloss:0.87137\n",
      "[8]\tval-logloss:0.83116\n",
      "[9]\tval-logloss:0.81455\n",
      "[10]\tval-logloss:0.79544\n",
      "[11]\tval-logloss:0.78471\n",
      "[12]\tval-logloss:0.77369\n",
      "[13]\tval-logloss:0.75087\n",
      "[14]\tval-logloss:0.73806\n",
      "[15]\tval-logloss:0.73581\n",
      "[16]\tval-logloss:0.72696\n",
      "[17]\tval-logloss:0.70609\n",
      "[18]\tval-logloss:0.69808\n",
      "[19]\tval-logloss:0.68590\n",
      "[20]\tval-logloss:0.67946\n",
      "[21]\tval-logloss:0.67706\n",
      "[22]\tval-logloss:0.66944\n",
      "[23]\tval-logloss:0.65499\n",
      "[24]\tval-logloss:0.64581\n",
      "[25]\tval-logloss:0.63769\n",
      "[26]\tval-logloss:0.62732\n",
      "[27]\tval-logloss:0.62360\n",
      "[28]\tval-logloss:0.61944\n",
      "[29]\tval-logloss:0.61664\n",
      "[30]\tval-logloss:0.61374\n",
      "[31]\tval-logloss:0.60660\n",
      "[32]\tval-logloss:0.59913\n",
      "[33]\tval-logloss:0.59618\n",
      "[34]\tval-logloss:0.59406\n",
      "[35]\tval-logloss:0.59072\n",
      "[36]\tval-logloss:0.58998\n",
      "[37]\tval-logloss:0.58936\n",
      "[38]\tval-logloss:0.58651\n",
      "[39]\tval-logloss:0.58224\n",
      "[40]\tval-logloss:0.58034\n",
      "[41]\tval-logloss:0.57821\n",
      "[42]\tval-logloss:0.57595\n",
      "[43]\tval-logloss:0.57440\n",
      "[44]\tval-logloss:0.57273\n",
      "[45]\tval-logloss:0.57129\n",
      "[46]\tval-logloss:0.56920\n",
      "[47]\tval-logloss:0.56392\n",
      "[48]\tval-logloss:0.56348\n",
      "[49]\tval-logloss:0.56280\n",
      "[50]\tval-logloss:0.56081\n",
      "[51]\tval-logloss:0.55964\n",
      "[52]\tval-logloss:0.55852\n",
      "[53]\tval-logloss:0.55820\n",
      "[54]\tval-logloss:0.55767\n",
      "[55]\tval-logloss:0.55664\n",
      "[56]\tval-logloss:0.55563\n",
      "[57]\tval-logloss:0.55508\n",
      "[58]\tval-logloss:0.55408\n",
      "[59]\tval-logloss:0.55353\n",
      "[60]\tval-logloss:0.55274\n",
      "[61]\tval-logloss:0.55291\n",
      "[62]\tval-logloss:0.55171\n",
      "[63]\tval-logloss:0.55039\n",
      "[64]\tval-logloss:0.55015\n",
      "[65]\tval-logloss:0.54859\n",
      "[66]\tval-logloss:0.54833\n",
      "[67]\tval-logloss:0.54813\n",
      "[68]\tval-logloss:0.54718\n",
      "[69]\tval-logloss:0.54680\n",
      "[70]\tval-logloss:0.54571\n",
      "[71]\tval-logloss:0.54470\n",
      "[72]\tval-logloss:0.54404\n",
      "[73]\tval-logloss:0.54092\n",
      "[74]\tval-logloss:0.54019\n",
      "[75]\tval-logloss:0.53976\n",
      "[76]\tval-logloss:0.53957\n",
      "[77]\tval-logloss:0.53934\n",
      "[78]\tval-logloss:0.53868\n",
      "[79]\tval-logloss:0.53806\n",
      "[80]\tval-logloss:0.53701\n",
      "[81]\tval-logloss:0.53658\n",
      "[82]\tval-logloss:0.53443\n",
      "[83]\tval-logloss:0.53387\n",
      "[84]\tval-logloss:0.53307\n",
      "[85]\tval-logloss:0.53234\n",
      "[86]\tval-logloss:0.53171\n",
      "[87]\tval-logloss:0.53031\n",
      "[88]\tval-logloss:0.52987\n",
      "[89]\tval-logloss:0.52920\n",
      "[90]\tval-logloss:0.52883\n",
      "[91]\tval-logloss:0.52850\n",
      "[92]\tval-logloss:0.52809\n",
      "[93]\tval-logloss:0.52813\n",
      "[94]\tval-logloss:0.52757\n",
      "[95]\tval-logloss:0.52676\n",
      "[96]\tval-logloss:0.52636\n",
      "[97]\tval-logloss:0.52506\n",
      "[98]\tval-logloss:0.52364\n",
      "[99]\tval-logloss:0.52331\n",
      "[100]\tval-logloss:0.52266\n",
      "[101]\tval-logloss:0.52186\n",
      "[102]\tval-logloss:0.52168\n",
      "[103]\tval-logloss:0.52087\n",
      "[104]\tval-logloss:0.52040\n",
      "[105]\tval-logloss:0.51982\n",
      "[106]\tval-logloss:0.51836\n",
      "[107]\tval-logloss:0.51808\n",
      "[108]\tval-logloss:0.51763\n",
      "[109]\tval-logloss:0.51752\n",
      "[110]\tval-logloss:0.51724\n",
      "[111]\tval-logloss:0.51679\n",
      "[112]\tval-logloss:0.51592\n",
      "[113]\tval-logloss:0.51635\n",
      "[114]\tval-logloss:0.51563\n",
      "[115]\tval-logloss:0.51529\n",
      "[116]\tval-logloss:0.51500\n",
      "[117]\tval-logloss:0.51447\n",
      "[118]\tval-logloss:0.51408\n",
      "[119]\tval-logloss:0.51335\n",
      "[120]\tval-logloss:0.51304\n",
      "[121]\tval-logloss:0.51307\n",
      "[122]\tval-logloss:0.51265\n",
      "[123]\tval-logloss:0.51214\n",
      "[124]\tval-logloss:0.51216\n",
      "[125]\tval-logloss:0.51189\n",
      "[126]\tval-logloss:0.51158\n",
      "[127]\tval-logloss:0.51147\n",
      "[128]\tval-logloss:0.51097\n",
      "[129]\tval-logloss:0.51030\n",
      "[130]\tval-logloss:0.51025\n",
      "[131]\tval-logloss:0.50977\n",
      "[132]\tval-logloss:0.50929\n",
      "[133]\tval-logloss:0.50919\n",
      "[134]\tval-logloss:0.50864\n",
      "[135]\tval-logloss:0.50769\n",
      "[136]\tval-logloss:0.50734\n",
      "[137]\tval-logloss:0.50704\n",
      "[138]\tval-logloss:0.50681\n",
      "[139]\tval-logloss:0.50609\n",
      "[140]\tval-logloss:0.50554\n",
      "[141]\tval-logloss:0.50494\n",
      "[142]\tval-logloss:0.50443\n",
      "[143]\tval-logloss:0.50451\n",
      "[144]\tval-logloss:0.50426\n",
      "[145]\tval-logloss:0.50383\n",
      "[146]\tval-logloss:0.50379\n",
      "[147]\tval-logloss:0.50402\n",
      "[148]\tval-logloss:0.50278\n",
      "[149]\tval-logloss:0.50271\n",
      "[150]\tval-logloss:0.50304\n",
      "[151]\tval-logloss:0.50302\n",
      "[152]\tval-logloss:0.50288\n",
      "[153]\tval-logloss:0.50294\n",
      "[154]\tval-logloss:0.50258\n",
      "[155]\tval-logloss:0.50238\n",
      "[156]\tval-logloss:0.50152\n",
      "[157]\tval-logloss:0.50114\n",
      "[158]\tval-logloss:0.50107\n",
      "[159]\tval-logloss:0.50067\n",
      "[160]\tval-logloss:0.50032\n",
      "[161]\tval-logloss:0.50036\n",
      "[162]\tval-logloss:0.49970\n",
      "[163]\tval-logloss:0.49933\n",
      "[164]\tval-logloss:0.49870\n",
      "[165]\tval-logloss:0.49853\n",
      "[166]\tval-logloss:0.49852\n",
      "[167]\tval-logloss:0.49811\n",
      "[168]\tval-logloss:0.49784\n",
      "[169]\tval-logloss:0.49779\n",
      "[170]\tval-logloss:0.49809\n",
      "[171]\tval-logloss:0.49792\n",
      "[172]\tval-logloss:0.49763\n",
      "[173]\tval-logloss:0.49723\n",
      "[174]\tval-logloss:0.49646\n",
      "[175]\tval-logloss:0.49629\n",
      "[176]\tval-logloss:0.49604\n",
      "[177]\tval-logloss:0.49600\n",
      "[178]\tval-logloss:0.49552\n",
      "[179]\tval-logloss:0.49576\n",
      "[180]\tval-logloss:0.49540\n",
      "[181]\tval-logloss:0.49553\n",
      "[182]\tval-logloss:0.49512\n",
      "[183]\tval-logloss:0.49498\n",
      "[184]\tval-logloss:0.49467\n",
      "Model saved successfully.\n",
      "Raw probabilities (sample): [0.03921884 0.5152173  0.02486663 0.75707746 0.34764296 0.34372362\n",
      " 0.53229594 0.17718077 0.05142022 0.5142168 ]\n",
      "\n",
      "Threshold: 0.6\n",
      "Accuracy: 0.8133\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89     11997\n",
      "           1       0.36      0.32      0.34      2095\n",
      "\n",
      "    accuracy                           0.81     14092\n",
      "   macro avg       0.62      0.61      0.61     14092\n",
      "weighted avg       0.80      0.81      0.81     14092\n",
      "\n",
      "\n",
      "Feature Importances:\n",
      "open (f0): 230.0\n",
      "high (f1): 152.0\n",
      "low (f2): 132.0\n",
      "close (f3): 124.0\n",
      "volume (f4): 544.0\n",
      "SMA_20 (f5): 172.0\n",
      "RSI_14 (f6): 498.0\n",
      "MACD (f7): 441.0\n",
      "ATR_14 (f8): 451.0\n",
      "OBV (f9): 633.0\n",
      "ROC_10 (f10): 636.0\n",
      "price_delta (f11): 438.0\n",
      "volatility (f12): 640.0\n",
      "RSI_lag_1 (f13): 544.0\n",
      "hour (f14): 878.0\n",
      "dayofweek (f15): 403.0\n",
      "ATR_ratio (f16): 698.0\n",
      "Volume_ratio (f17): 686.0\n",
      "MACD_signal (f18): 529.0\n",
      "high_low_spread (f19): 613.0\n",
      "open_close_diff (f20): 353.0\n",
      "macd (f21): 76.0\n",
      "trend (f22): 535.0\n",
      "price_vol_ratio (f23): 544.0\n",
      "mom_div (f24): 750.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main execution\n",
    "df = load_data(\"D:/try-quant/processed_data_version_7.csv\")\n",
    "df = feature_engineering(df)\n",
    "df = define_target(df)\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in ['target', 'datetime']]\n",
    "print(\"Feature columns:\", feature_cols)\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "\n",
    "print(\"Original class distribution:\", np.bincount(y))\n",
    "\n",
    "X_res, y_res = undersample_majority(X, y, target_size=60000)\n",
    "print(\"Class distribution after undersampling:\", np.bincount(y_res))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_res)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_res, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train_orig, X_val, y_train_orig, y_val = train_test_split(X_temp, y_temp, test_size=0.25, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Train (before SMOTE) class distribution:\", np.bincount(y_train_orig))\n",
    "print(\"Validation class distribution:\", np.bincount(y_val))\n",
    "print(\"Test class distribution:\", np.bincount(y_test))\n",
    "\n",
    "smote = SMOTE(sampling_strategy={0: 36044, 1: 30000}, random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "print(\"Train class distribution (after SMOTE):\", np.bincount(y_train))\n",
    "\n",
    "best_params = optimize_xgboost(X_train, y_train, X_val, y_val)\n",
    "model = train_model(X_train, y_train, X_val, y_val, best_params)\n",
    "evaluate_model(model, best_params, X_train, y_train, X_val, y_val, X_test, y_test, feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
