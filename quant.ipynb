{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using single threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "def load_data(filepath):\n",
    "    df = pd.read_csv(filepath, parse_dates=['datetime'], index_col='datetime')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def feature_engineering(df):\n",
    "    df['SMA_20'] = df.ta.sma(length=20)\n",
    "    df['RSI_14'] = df.ta.rsi(length=14)\n",
    "    df['rsi_mom'] = df['RSI_14'] - df['RSI_14'].shift(5)\n",
    "    macd = df.ta.macd()\n",
    "    df['MACD'] = macd['MACD_12_26_9']\n",
    "    df['MACD_signal'] = macd['MACDs_12_26_9']\n",
    "    df['MACD_hist'] = df['MACD'] - df['MACD_signal']\n",
    "    df['ATR_14'] = df.ta.atr(length=14)\n",
    "    df['volume_SMA_20'] = df['volume'].rolling(20).mean()\n",
    "    df['OBV'] = ta.obv(df['close'], df['volume'])\n",
    "    df['ROC_10'] = df['close'].pct_change(10)\n",
    "    df['lagged_return'] = df['close'].pct_change().shift(1)\n",
    "    bbands = df.ta.bbands()\n",
    "    df['BB_upper'] = bbands['BBU_5_2.0']\n",
    "    df['BB_lower'] = bbands['BBL_5_2.0']\n",
    "    df['momentum'] = df['close'].diff(5)\n",
    "    df['volatility'] = df['close'].rolling(20).std()\n",
    "    df['vol_ratio'] = df['volatility'] / df['volatility'].shift(5)\n",
    "    df['trend'] = df['close'] / df['SMA_20']\n",
    "    df['acceleration'] = df['close'].diff(5).diff(5)\n",
    "    df['rsi_div'] = df['RSI_14'].diff()\n",
    "    df['vol_spike'] = np.where(df['volume_SMA_20'] == 0, 0, df['volume'] / df['volume_SMA_20'])\n",
    "    df['price_vol_ratio'] = np.where(df['volume'] == 0, 0, df['close'] / df['volume'])\n",
    "    df['mom_div'] = df['momentum'] - df['momentum'].shift(5)\n",
    "    df.dropna(inplace=True)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target\n",
    "def define_target(df):\n",
    "    df['target'] = np.where(df['close'].shift(-1) > df['close'] * 1.005, 1, 0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample majority class\n",
    "def undersample_majority(X, y, target_size=60000):\n",
    "    rus = RandomUnderSampler(sampling_strategy={0: target_size, 1: np.sum(y == 1)}, random_state=42)\n",
    "    X_res, y_res = rus.fit_resample(X, y)\n",
    "    return X_res, y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize xgoost\n",
    "def optimize_xgboost(X_train, y_train, X_val, y_val):\n",
    "    def xgb_eval(max_depth, learning_rate, colsample_bytree, subsample, gamma, min_child_weight, num_boost_round):\n",
    "        params = {\n",
    "            'max_depth': int(max_depth),\n",
    "            'learning_rate': learning_rate,\n",
    "            'colsample_bytree': colsample_bytree,\n",
    "            'subsample': subsample,\n",
    "            'gamma': gamma,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'eval_metric': 'logloss',\n",
    "            'scale_pos_weight': 3.0, \n",
    "            'objective': 'binary:logistic'\n",
    "        }\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        model = xgb.train(params, dtrain, num_boost_round=int(num_boost_round), evals=[(dval, 'val')], \n",
    "                          early_stopping_rounds=10, verbose_eval=False)\n",
    "        return -model.best_score\n",
    "    \n",
    "    param_bounds = {\n",
    "        'max_depth': (3, 6),\n",
    "        'learning_rate': (0.05, 0.5),\n",
    "        'colsample_bytree': (0.6, 1.0),\n",
    "        'subsample': (0.6, 1.0),\n",
    "        'gamma': (0, 5),\n",
    "        'min_child_weight': (5, 10),\n",
    "        'num_boost_round': (50, 300)\n",
    "    }\n",
    "    optimizer = BayesianOptimization(f=xgb_eval, pbounds=param_bounds, random_state=42)\n",
    "    optimizer.maximize(n_iter=30)\n",
    "    \n",
    "    best_params = optimizer.max['params']\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['num_boost_round'] = int(best_params['num_boost_round'])\n",
    "    best_params['min_child_weight'] = int(best_params['min_child_weight'])\n",
    "    best_params['eval_metric'] = 'logloss'\n",
    "    best_params['scale_pos_weight'] = 3.0 \n",
    "    best_params['objective'] = 'binary:logistic'\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_model(X_train, y_train, X_val, y_val, best_params):\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    params = {k: v for k, v in best_params.items() if k != 'num_boost_round'}\n",
    "    print(\"Training with parameters:\", params)\n",
    "    model = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=best_params['num_boost_round'],\n",
    "        evals=[(dval, 'val')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=True\n",
    "    )\n",
    "    joblib.dump(model, \"optimized_xgboost.joblib\")\n",
    "    print(\"Model saved successfully.\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with single threshold\n",
    "def evaluate_model(model, best_params, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    params = {k: v for k, v in best_params.items() if k not in ['eval_metric', 'num_boost_round', 'objective']}\n",
    "    xgb_clf = XGBClassifier(**params, n_estimators=model.best_iteration + 1)\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    calibrated = CalibratedClassifierCV(xgb_clf, method='isotonic', cv='prefit')\n",
    "    calibrated.fit(X_val, y_val)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    y_probs_raw = np.clip(model.predict(dtest), 0, 1)\n",
    "    y_probs_calibrated = calibrated.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    print(\"Raw probabilities (sample):\", y_probs_raw[:10])\n",
    "    print(\"Calibrated probabilities (sample):\", y_probs_calibrated[:10])\n",
    "    \n",
    "    lr = LogisticRegression(class_weight={0: 1, 1: 1}, penalty='l1', solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_probs_lr = lr.predict_proba(X_test)[:, 1]\n",
    "    y_probs_ensemble = 0.9 * y_probs_calibrated + 0.1 * y_probs_lr\n",
    "    \n",
    "    threshold = 0.38\n",
    "    y_pred = (y_probs_ensemble > threshold).astype(int)\n",
    "    print(f\"\\nThreshold: {threshold}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: [83371 10459]\n",
      "Class distribution after undersampling: [60000 10459]\n",
      "Train (before SMOTE) class distribution: [36044  6231]\n",
      "Validation class distribution: [11959  2133]\n",
      "Test class distribution: [11997  2095]\n",
      "Train class distribution (after SMOTE): [36044 28000]\n",
      "|   iter    |  target   | colsam... |   gamma   | learni... | max_depth | min_ch... | num_bo... | subsample |\n",
      "-------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-0.5336  \u001b[39m | \u001b[39m0.7498   \u001b[39m | \u001b[39m4.754    \u001b[39m | \u001b[39m0.3794   \u001b[39m | \u001b[39m4.796    \u001b[39m | \u001b[39m5.78     \u001b[39m | \u001b[39m89.0     \u001b[39m | \u001b[39m0.6232   \u001b[39m |\n",
      "| \u001b[35m2        \u001b[39m | \u001b[35m-0.509   \u001b[39m | \u001b[35m0.9465   \u001b[39m | \u001b[35m3.006    \u001b[39m | \u001b[35m0.3686   \u001b[39m | \u001b[35m3.062    \u001b[39m | \u001b[35m9.85     \u001b[39m | \u001b[35m258.1    \u001b[39m | \u001b[35m0.6849   \u001b[39m |\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-0.5358  \u001b[39m | \u001b[39m0.6727   \u001b[39m | \u001b[39m0.917    \u001b[39m | \u001b[39m0.1869   \u001b[39m | \u001b[39m4.574    \u001b[39m | \u001b[39m7.16     \u001b[39m | \u001b[39m122.8    \u001b[39m | \u001b[39m0.8447   \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-0.5463  \u001b[39m | \u001b[39m0.6558   \u001b[39m | \u001b[39m1.461    \u001b[39m | \u001b[39m0.2149   \u001b[39m | \u001b[39m4.368    \u001b[39m | \u001b[39m8.926    \u001b[39m | \u001b[39m99.92    \u001b[39m | \u001b[39m0.8057   \u001b[39m |\n",
      "| \u001b[35m5        \u001b[39m | \u001b[35m-0.5002  \u001b[39m | \u001b[35m0.837    \u001b[39m | \u001b[35m0.2323   \u001b[39m | \u001b[35m0.3234   \u001b[39m | \u001b[35m3.512    \u001b[39m | \u001b[35m5.325    \u001b[39m | \u001b[35m287.2    \u001b[39m | \u001b[35m0.9863   \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m-0.4981  \u001b[39m | \u001b[35m0.997    \u001b[39m | \u001b[35m4.895    \u001b[39m | \u001b[35m0.4094   \u001b[39m | \u001b[35m4.028    \u001b[39m | \u001b[35m9.686    \u001b[39m | \u001b[35m299.7    \u001b[39m | \u001b[35m0.7849   \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-0.5033  \u001b[39m | \u001b[39m0.9737   \u001b[39m | \u001b[39m4.079    \u001b[39m | \u001b[39m0.4447   \u001b[39m | \u001b[39m4.317    \u001b[39m | \u001b[39m8.582    \u001b[39m | \u001b[39m299.6    \u001b[39m | \u001b[39m0.773    \u001b[39m |\n",
      "| \u001b[39m8        \u001b[39m | \u001b[39m-0.5162  \u001b[39m | \u001b[39m0.6408   \u001b[39m | \u001b[39m4.488    \u001b[39m | \u001b[39m0.2535   \u001b[39m | \u001b[39m3.491    \u001b[39m | \u001b[39m9.856    \u001b[39m | \u001b[39m276.3    \u001b[39m | \u001b[39m0.71     \u001b[39m |\n",
      "| \u001b[39m9        \u001b[39m | \u001b[39m-0.5228  \u001b[39m | \u001b[39m0.6563   \u001b[39m | \u001b[39m4.882    \u001b[39m | \u001b[39m0.271    \u001b[39m | \u001b[39m4.768    \u001b[39m | \u001b[39m9.976    \u001b[39m | \u001b[39m291.8    \u001b[39m | \u001b[39m0.9746   \u001b[39m |\n",
      "| \u001b[35m10       \u001b[39m | \u001b[35m-0.4825  \u001b[39m | \u001b[35m0.8375   \u001b[39m | \u001b[35m2.173    \u001b[39m | \u001b[35m0.4117   \u001b[39m | \u001b[35m5.131    \u001b[39m | \u001b[35m5.005    \u001b[39m | \u001b[35m282.6    \u001b[39m | \u001b[35m0.8999   \u001b[39m |\n",
      "| \u001b[39m11       \u001b[39m | \u001b[39m-0.5142  \u001b[39m | \u001b[39m0.6563   \u001b[39m | \u001b[39m0.3748   \u001b[39m | \u001b[39m0.419    \u001b[39m | \u001b[39m3.067    \u001b[39m | \u001b[39m5.103    \u001b[39m | \u001b[39m280.4    \u001b[39m | \u001b[39m0.9374   \u001b[39m |\n",
      "| \u001b[35m12       \u001b[39m | \u001b[35m-0.4818  \u001b[39m | \u001b[35m0.6007   \u001b[39m | \u001b[35m3.419    \u001b[39m | \u001b[35m0.3071   \u001b[39m | \u001b[35m5.583    \u001b[39m | \u001b[35m6.459    \u001b[39m | \u001b[35m284.8    \u001b[39m | \u001b[35m0.8767   \u001b[39m |\n",
      "| \u001b[39m13       \u001b[39m | \u001b[39m-0.4976  \u001b[39m | \u001b[39m0.6467   \u001b[39m | \u001b[39m4.768    \u001b[39m | \u001b[39m0.4386   \u001b[39m | \u001b[39m5.927    \u001b[39m | \u001b[39m6.365    \u001b[39m | \u001b[39m280.9    \u001b[39m | \u001b[39m0.811    \u001b[39m |\n",
      "| \u001b[35m14       \u001b[39m | \u001b[35m-0.4664  \u001b[39m | \u001b[35m0.7533   \u001b[39m | \u001b[35m0.003325 \u001b[39m | \u001b[35m0.27     \u001b[39m | \u001b[35m5.949    \u001b[39m | \u001b[35m8.871    \u001b[39m | \u001b[35m284.2    \u001b[39m | \u001b[35m0.9121   \u001b[39m |\n",
      "| \u001b[39m15       \u001b[39m | \u001b[39m-0.4871  \u001b[39m | \u001b[39m0.9243   \u001b[39m | \u001b[39m1.735    \u001b[39m | \u001b[39m0.3547   \u001b[39m | \u001b[39m4.966    \u001b[39m | \u001b[39m9.888    \u001b[39m | \u001b[39m283.8    \u001b[39m | \u001b[39m0.7707   \u001b[39m |\n",
      "| \u001b[39m16       \u001b[39m | \u001b[39m-0.4915  \u001b[39m | \u001b[39m0.6398   \u001b[39m | \u001b[39m0.3099   \u001b[39m | \u001b[39m0.478    \u001b[39m | \u001b[39m5.861    \u001b[39m | \u001b[39m5.989    \u001b[39m | \u001b[39m284.3    \u001b[39m | \u001b[39m0.9008   \u001b[39m |\n",
      "| \u001b[39m17       \u001b[39m | \u001b[39m-0.5014  \u001b[39m | \u001b[39m0.8397   \u001b[39m | \u001b[39m0.233    \u001b[39m | \u001b[39m0.1865   \u001b[39m | \u001b[39m4.962    \u001b[39m | \u001b[39m9.806    \u001b[39m | \u001b[39m287.6    \u001b[39m | \u001b[39m0.6394   \u001b[39m |\n",
      "| \u001b[39m18       \u001b[39m | \u001b[39m-0.4814  \u001b[39m | \u001b[39m0.8203   \u001b[39m | \u001b[39m0.06691  \u001b[39m | \u001b[39m0.4309   \u001b[39m | \u001b[39m5.718    \u001b[39m | \u001b[39m8.686    \u001b[39m | \u001b[39m282.0    \u001b[39m | \u001b[39m0.9392   \u001b[39m |\n",
      "| \u001b[39m19       \u001b[39m | \u001b[39m-0.5182  \u001b[39m | \u001b[39m0.8658   \u001b[39m | \u001b[39m3.965    \u001b[39m | \u001b[39m0.2815   \u001b[39m | \u001b[39m3.451    \u001b[39m | \u001b[39m5.063    \u001b[39m | \u001b[39m284.8    \u001b[39m | \u001b[39m0.9345   \u001b[39m |\n",
      "| \u001b[39m20       \u001b[39m | \u001b[39m-0.4853  \u001b[39m | \u001b[39m0.6555   \u001b[39m | \u001b[39m1.56     \u001b[39m | \u001b[39m0.2823   \u001b[39m | \u001b[39m5.971    \u001b[39m | \u001b[39m7.389    \u001b[39m | \u001b[39m283.8    \u001b[39m | \u001b[39m0.6161   \u001b[39m |\n",
      "| \u001b[39m21       \u001b[39m | \u001b[39m-0.515   \u001b[39m | \u001b[39m0.8974   \u001b[39m | \u001b[39m0.03655  \u001b[39m | \u001b[39m0.1307   \u001b[39m | \u001b[39m4.304    \u001b[39m | \u001b[39m9.124    \u001b[39m | \u001b[39m284.0    \u001b[39m | \u001b[39m0.7919   \u001b[39m |\n",
      "| \u001b[39m22       \u001b[39m | \u001b[39m-0.4782  \u001b[39m | \u001b[39m0.8831   \u001b[39m | \u001b[39m1.281    \u001b[39m | \u001b[39m0.1949   \u001b[39m | \u001b[39m5.941    \u001b[39m | \u001b[39m8.666    \u001b[39m | \u001b[39m285.6    \u001b[39m | \u001b[39m0.7078   \u001b[39m |\n",
      "| \u001b[39m23       \u001b[39m | \u001b[39m-0.4907  \u001b[39m | \u001b[39m0.9608   \u001b[39m | \u001b[39m3.909    \u001b[39m | \u001b[39m0.1412   \u001b[39m | \u001b[39m5.903    \u001b[39m | \u001b[39m8.418    \u001b[39m | \u001b[39m285.9    \u001b[39m | \u001b[39m0.9103   \u001b[39m |\n",
      "| \u001b[39m24       \u001b[39m | \u001b[39m-0.4826  \u001b[39m | \u001b[39m0.6455   \u001b[39m | \u001b[39m3.215    \u001b[39m | \u001b[39m0.4579   \u001b[39m | \u001b[39m5.993    \u001b[39m | \u001b[39m8.525    \u001b[39m | \u001b[39m283.3    \u001b[39m | \u001b[39m0.885    \u001b[39m |\n",
      "| \u001b[39m25       \u001b[39m | \u001b[39m-0.4769  \u001b[39m | \u001b[39m0.8524   \u001b[39m | \u001b[39m1.569    \u001b[39m | \u001b[39m0.1727   \u001b[39m | \u001b[39m5.961    \u001b[39m | \u001b[39m9.583    \u001b[39m | \u001b[39m280.9    \u001b[39m | \u001b[39m0.9018   \u001b[39m |\n",
      "| \u001b[39m26       \u001b[39m | \u001b[39m-0.4834  \u001b[39m | \u001b[39m0.8977   \u001b[39m | \u001b[39m3.616    \u001b[39m | \u001b[39m0.1868   \u001b[39m | \u001b[39m5.836    \u001b[39m | \u001b[39m9.708    \u001b[39m | \u001b[39m281.3    \u001b[39m | \u001b[39m0.9106   \u001b[39m |\n",
      "| \u001b[39m27       \u001b[39m | \u001b[39m-0.4975  \u001b[39m | \u001b[39m0.8145   \u001b[39m | \u001b[39m1.337    \u001b[39m | \u001b[39m0.399    \u001b[39m | \u001b[39m5.895    \u001b[39m | \u001b[39m7.402    \u001b[39m | \u001b[39m278.8    \u001b[39m | \u001b[39m0.6905   \u001b[39m |\n",
      "| \u001b[39m28       \u001b[39m | \u001b[39m-0.5677  \u001b[39m | \u001b[39m0.6593   \u001b[39m | \u001b[39m1.552    \u001b[39m | \u001b[39m0.0695   \u001b[39m | \u001b[39m4.55     \u001b[39m | \u001b[39m8.237    \u001b[39m | \u001b[39m221.3    \u001b[39m | \u001b[39m0.8039   \u001b[39m |\n",
      "| \u001b[39m29       \u001b[39m | \u001b[39m-0.4871  \u001b[39m | \u001b[39m0.9604   \u001b[39m | \u001b[39m2.366    \u001b[39m | \u001b[39m0.2512   \u001b[39m | \u001b[39m5.084    \u001b[39m | \u001b[39m5.508    \u001b[39m | \u001b[39m172.4    \u001b[39m | \u001b[39m0.905    \u001b[39m |\n",
      "| \u001b[39m30       \u001b[39m | \u001b[39m-0.508   \u001b[39m | \u001b[39m0.6324   \u001b[39m | \u001b[39m2.045    \u001b[39m | \u001b[39m0.1639   \u001b[39m | \u001b[39m5.018    \u001b[39m | \u001b[39m9.491    \u001b[39m | \u001b[39m170.8    \u001b[39m | \u001b[39m0.6577   \u001b[39m |\n",
      "| \u001b[39m31       \u001b[39m | \u001b[39m-0.5144  \u001b[39m | \u001b[39m0.9822   \u001b[39m | \u001b[39m4.142    \u001b[39m | \u001b[39m0.1959   \u001b[39m | \u001b[39m5.176    \u001b[39m | \u001b[39m5.468    \u001b[39m | \u001b[39m175.8    \u001b[39m | \u001b[39m0.9977   \u001b[39m |\n",
      "| \u001b[39m32       \u001b[39m | \u001b[39m-0.5217  \u001b[39m | \u001b[39m0.8296   \u001b[39m | \u001b[39m4.704    \u001b[39m | \u001b[39m0.206    \u001b[39m | \u001b[39m4.226    \u001b[39m | \u001b[39m5.002    \u001b[39m | \u001b[39m169.4    \u001b[39m | \u001b[39m0.703    \u001b[39m |\n",
      "| \u001b[39m33       \u001b[39m | \u001b[39m-0.5253  \u001b[39m | \u001b[39m0.9402   \u001b[39m | \u001b[39m0.2404   \u001b[39m | \u001b[39m0.2693   \u001b[39m | \u001b[39m3.195    \u001b[39m | \u001b[39m5.171    \u001b[39m | \u001b[39m172.8    \u001b[39m | \u001b[39m0.9645   \u001b[39m |\n",
      "| \u001b[39m34       \u001b[39m | \u001b[39m-0.7996  \u001b[39m | \u001b[39m0.6707   \u001b[39m | \u001b[39m1.213    \u001b[39m | \u001b[39m0.05678  \u001b[39m | \u001b[39m4.062    \u001b[39m | \u001b[39m7.984    \u001b[39m | \u001b[39m50.26    \u001b[39m | \u001b[39m0.6643   \u001b[39m |\n",
      "| \u001b[39m35       \u001b[39m | \u001b[39m-0.602   \u001b[39m | \u001b[39m0.6445   \u001b[39m | \u001b[39m4.428    \u001b[39m | \u001b[39m0.05531  \u001b[39m | \u001b[39m5.964    \u001b[39m | \u001b[39m9.913    \u001b[39m | \u001b[39m145.2    \u001b[39m | \u001b[39m0.6683   \u001b[39m |\n",
      "=============================================================================================================\n",
      "Training with parameters: {'colsample_bytree': 0.7533498344547301, 'gamma': 0.0033250401019507825, 'learning_rate': 0.269979110698731, 'max_depth': 5, 'min_child_weight': 8, 'subsample': 0.912149616655841, 'eval_metric': 'logloss', 'scale_pos_weight': 3.0, 'objective': 'binary:logistic'}\n",
      "[0]\tval-logloss:0.95362\n",
      "[1]\tval-logloss:0.90770\n",
      "[2]\tval-logloss:0.87274\n",
      "[3]\tval-logloss:0.85887\n",
      "[4]\tval-logloss:0.84538\n",
      "[5]\tval-logloss:0.80738\n",
      "[6]\tval-logloss:0.80438\n",
      "[7]\tval-logloss:0.79831\n",
      "[8]\tval-logloss:0.79162\n",
      "[9]\tval-logloss:0.77708\n",
      "[10]\tval-logloss:0.74965\n",
      "[11]\tval-logloss:0.73987\n",
      "[12]\tval-logloss:0.72594\n",
      "[13]\tval-logloss:0.71364\n",
      "[14]\tval-logloss:0.70574\n",
      "[15]\tval-logloss:0.67850\n",
      "[16]\tval-logloss:0.67248\n",
      "[17]\tval-logloss:0.65413\n",
      "[18]\tval-logloss:0.64564\n",
      "[19]\tval-logloss:0.64603\n",
      "[20]\tval-logloss:0.63815\n",
      "[21]\tval-logloss:0.63550\n",
      "[22]\tval-logloss:0.62960\n",
      "[23]\tval-logloss:0.62533\n",
      "[24]\tval-logloss:0.61828\n",
      "[25]\tval-logloss:0.61121\n",
      "[26]\tval-logloss:0.60772\n",
      "[27]\tval-logloss:0.60519\n",
      "[28]\tval-logloss:0.60366\n",
      "[29]\tval-logloss:0.59853\n",
      "[30]\tval-logloss:0.59510\n",
      "[31]\tval-logloss:0.59023\n",
      "[32]\tval-logloss:0.59007\n",
      "[33]\tval-logloss:0.58283\n",
      "[34]\tval-logloss:0.58128\n",
      "[35]\tval-logloss:0.57543\n",
      "[36]\tval-logloss:0.57349\n",
      "[37]\tval-logloss:0.56943\n",
      "[38]\tval-logloss:0.56878\n",
      "[39]\tval-logloss:0.56790\n",
      "[40]\tval-logloss:0.56397\n",
      "[41]\tval-logloss:0.56163\n",
      "[42]\tval-logloss:0.56184\n",
      "[43]\tval-logloss:0.56146\n",
      "[44]\tval-logloss:0.56038\n",
      "[45]\tval-logloss:0.56032\n",
      "[46]\tval-logloss:0.55509\n",
      "[47]\tval-logloss:0.54837\n",
      "[48]\tval-logloss:0.54661\n",
      "[49]\tval-logloss:0.54592\n",
      "[50]\tval-logloss:0.54542\n",
      "[51]\tval-logloss:0.54538\n",
      "[52]\tval-logloss:0.54233\n",
      "[53]\tval-logloss:0.54233\n",
      "[54]\tval-logloss:0.54160\n",
      "[55]\tval-logloss:0.54099\n",
      "[56]\tval-logloss:0.53968\n",
      "[57]\tval-logloss:0.53906\n",
      "[58]\tval-logloss:0.53655\n",
      "[59]\tval-logloss:0.53592\n",
      "[60]\tval-logloss:0.53502\n",
      "[61]\tval-logloss:0.53378\n",
      "[62]\tval-logloss:0.53200\n",
      "[63]\tval-logloss:0.53148\n",
      "[64]\tval-logloss:0.53092\n",
      "[65]\tval-logloss:0.52843\n",
      "[66]\tval-logloss:0.52861\n",
      "[67]\tval-logloss:0.52539\n",
      "[68]\tval-logloss:0.52488\n",
      "[69]\tval-logloss:0.52495\n",
      "[70]\tval-logloss:0.52423\n",
      "[71]\tval-logloss:0.52387\n",
      "[72]\tval-logloss:0.52294\n",
      "[73]\tval-logloss:0.52044\n",
      "[74]\tval-logloss:0.51954\n",
      "[75]\tval-logloss:0.51932\n",
      "[76]\tval-logloss:0.51886\n",
      "[77]\tval-logloss:0.51810\n",
      "[78]\tval-logloss:0.51815\n",
      "[79]\tval-logloss:0.51826\n",
      "[80]\tval-logloss:0.51812\n",
      "[81]\tval-logloss:0.51811\n",
      "[82]\tval-logloss:0.51469\n",
      "[83]\tval-logloss:0.51466\n",
      "[84]\tval-logloss:0.51473\n",
      "[85]\tval-logloss:0.51437\n",
      "[86]\tval-logloss:0.51389\n",
      "[87]\tval-logloss:0.51266\n",
      "[88]\tval-logloss:0.51152\n",
      "[89]\tval-logloss:0.51093\n",
      "[90]\tval-logloss:0.50913\n",
      "[91]\tval-logloss:0.50955\n",
      "[92]\tval-logloss:0.50890\n",
      "[93]\tval-logloss:0.50849\n",
      "[94]\tval-logloss:0.50829\n",
      "[95]\tval-logloss:0.50802\n",
      "[96]\tval-logloss:0.50723\n",
      "[97]\tval-logloss:0.50512\n",
      "[98]\tval-logloss:0.50509\n",
      "[99]\tval-logloss:0.50456\n",
      "[100]\tval-logloss:0.50404\n",
      "[101]\tval-logloss:0.50398\n",
      "[102]\tval-logloss:0.50389\n",
      "[103]\tval-logloss:0.50381\n",
      "[104]\tval-logloss:0.50230\n",
      "[105]\tval-logloss:0.50171\n",
      "[106]\tval-logloss:0.50129\n",
      "[107]\tval-logloss:0.50123\n",
      "[108]\tval-logloss:0.50016\n",
      "[109]\tval-logloss:0.49960\n",
      "[110]\tval-logloss:0.49937\n",
      "[111]\tval-logloss:0.49899\n",
      "[112]\tval-logloss:0.49765\n",
      "[113]\tval-logloss:0.49612\n",
      "[114]\tval-logloss:0.49577\n",
      "[115]\tval-logloss:0.49587\n",
      "[116]\tval-logloss:0.49632\n",
      "[117]\tval-logloss:0.49622\n",
      "[118]\tval-logloss:0.49590\n",
      "[119]\tval-logloss:0.49530\n",
      "[120]\tval-logloss:0.49521\n",
      "[121]\tval-logloss:0.49525\n",
      "[122]\tval-logloss:0.49524\n",
      "[123]\tval-logloss:0.49489\n",
      "[124]\tval-logloss:0.49467\n",
      "[125]\tval-logloss:0.49455\n",
      "[126]\tval-logloss:0.49389\n",
      "[127]\tval-logloss:0.49393\n",
      "[128]\tval-logloss:0.49404\n",
      "[129]\tval-logloss:0.49303\n",
      "[130]\tval-logloss:0.49241\n",
      "[131]\tval-logloss:0.49223\n",
      "[132]\tval-logloss:0.49214\n",
      "[133]\tval-logloss:0.49201\n",
      "[134]\tval-logloss:0.49195\n",
      "[135]\tval-logloss:0.49177\n",
      "[136]\tval-logloss:0.49186\n",
      "[137]\tval-logloss:0.49142\n",
      "[138]\tval-logloss:0.49147\n",
      "[139]\tval-logloss:0.49095\n",
      "[140]\tval-logloss:0.49051\n",
      "[141]\tval-logloss:0.49074\n",
      "[142]\tval-logloss:0.49046\n",
      "[143]\tval-logloss:0.49043\n",
      "[144]\tval-logloss:0.49007\n",
      "[145]\tval-logloss:0.48970\n",
      "[146]\tval-logloss:0.48958\n",
      "[147]\tval-logloss:0.48949\n",
      "[148]\tval-logloss:0.48934\n",
      "[149]\tval-logloss:0.48879\n",
      "[150]\tval-logloss:0.48834\n",
      "[151]\tval-logloss:0.48856\n",
      "[152]\tval-logloss:0.48789\n",
      "[153]\tval-logloss:0.48780\n",
      "[154]\tval-logloss:0.48790\n",
      "[155]\tval-logloss:0.48762\n",
      "[156]\tval-logloss:0.48773\n",
      "[157]\tval-logloss:0.48728\n",
      "[158]\tval-logloss:0.48711\n",
      "[159]\tval-logloss:0.48708\n",
      "[160]\tval-logloss:0.48580\n",
      "[161]\tval-logloss:0.48608\n",
      "[162]\tval-logloss:0.48582\n",
      "[163]\tval-logloss:0.48579\n",
      "[164]\tval-logloss:0.48592\n",
      "[165]\tval-logloss:0.48490\n",
      "[166]\tval-logloss:0.48462\n",
      "[167]\tval-logloss:0.48472\n",
      "[168]\tval-logloss:0.48497\n",
      "[169]\tval-logloss:0.48485\n",
      "[170]\tval-logloss:0.48500\n",
      "[171]\tval-logloss:0.48439\n",
      "[172]\tval-logloss:0.48400\n",
      "[173]\tval-logloss:0.48399\n",
      "[174]\tval-logloss:0.48345\n",
      "[175]\tval-logloss:0.48342\n",
      "[176]\tval-logloss:0.48326\n",
      "[177]\tval-logloss:0.48325\n",
      "[178]\tval-logloss:0.48294\n",
      "[179]\tval-logloss:0.48310\n",
      "[180]\tval-logloss:0.48284\n",
      "[181]\tval-logloss:0.48302\n",
      "[182]\tval-logloss:0.48294\n",
      "[183]\tval-logloss:0.48300\n",
      "[184]\tval-logloss:0.48337\n",
      "[185]\tval-logloss:0.48326\n",
      "[186]\tval-logloss:0.48294\n",
      "[187]\tval-logloss:0.48228\n",
      "[188]\tval-logloss:0.48216\n",
      "[189]\tval-logloss:0.48192\n",
      "[190]\tval-logloss:0.48177\n",
      "[191]\tval-logloss:0.48173\n",
      "[192]\tval-logloss:0.48123\n",
      "[193]\tval-logloss:0.48116\n",
      "[194]\tval-logloss:0.48135\n",
      "[195]\tval-logloss:0.48099\n",
      "[196]\tval-logloss:0.48106\n",
      "[197]\tval-logloss:0.48138\n",
      "[198]\tval-logloss:0.48130\n",
      "[199]\tval-logloss:0.48110\n",
      "[200]\tval-logloss:0.48056\n",
      "[201]\tval-logloss:0.48026\n",
      "[202]\tval-logloss:0.47996\n",
      "[203]\tval-logloss:0.48008\n",
      "[204]\tval-logloss:0.47981\n",
      "[205]\tval-logloss:0.47962\n",
      "[206]\tval-logloss:0.47943\n",
      "[207]\tval-logloss:0.47905\n",
      "[208]\tval-logloss:0.47885\n",
      "[209]\tval-logloss:0.47895\n",
      "[210]\tval-logloss:0.47877\n",
      "[211]\tval-logloss:0.47851\n",
      "[212]\tval-logloss:0.47797\n",
      "[213]\tval-logloss:0.47791\n",
      "[214]\tval-logloss:0.47773\n",
      "[215]\tval-logloss:0.47773\n",
      "[216]\tval-logloss:0.47751\n",
      "[217]\tval-logloss:0.47743\n",
      "[218]\tval-logloss:0.47708\n",
      "[219]\tval-logloss:0.47664\n",
      "[220]\tval-logloss:0.47658\n",
      "[221]\tval-logloss:0.47639\n",
      "[222]\tval-logloss:0.47640\n",
      "[223]\tval-logloss:0.47639\n",
      "[224]\tval-logloss:0.47647\n",
      "[225]\tval-logloss:0.47675\n",
      "[226]\tval-logloss:0.47649\n",
      "[227]\tval-logloss:0.47643\n",
      "[228]\tval-logloss:0.47631\n",
      "[229]\tval-logloss:0.47611\n",
      "[230]\tval-logloss:0.47563\n",
      "[231]\tval-logloss:0.47539\n",
      "[232]\tval-logloss:0.47555\n",
      "[233]\tval-logloss:0.47553\n",
      "[234]\tval-logloss:0.47525\n",
      "[235]\tval-logloss:0.47529\n",
      "[236]\tval-logloss:0.47471\n",
      "[237]\tval-logloss:0.47444\n",
      "[238]\tval-logloss:0.47439\n",
      "[239]\tval-logloss:0.47404\n",
      "[240]\tval-logloss:0.47391\n",
      "[241]\tval-logloss:0.47411\n",
      "[242]\tval-logloss:0.47375\n",
      "[243]\tval-logloss:0.47325\n",
      "[244]\tval-logloss:0.47324\n",
      "[245]\tval-logloss:0.47314\n",
      "[246]\tval-logloss:0.47295\n",
      "[247]\tval-logloss:0.47220\n",
      "[248]\tval-logloss:0.47236\n",
      "[249]\tval-logloss:0.47236\n",
      "[250]\tval-logloss:0.47247\n",
      "[251]\tval-logloss:0.47235\n",
      "[252]\tval-logloss:0.47248\n",
      "[253]\tval-logloss:0.47243\n",
      "[254]\tval-logloss:0.47206\n",
      "[255]\tval-logloss:0.47190\n",
      "[256]\tval-logloss:0.47186\n",
      "[257]\tval-logloss:0.47183\n",
      "[258]\tval-logloss:0.47170\n",
      "[259]\tval-logloss:0.47160\n",
      "[260]\tval-logloss:0.47141\n",
      "[261]\tval-logloss:0.47103\n",
      "[262]\tval-logloss:0.47082\n",
      "[263]\tval-logloss:0.47072\n",
      "[264]\tval-logloss:0.47021\n",
      "[265]\tval-logloss:0.47054\n",
      "[266]\tval-logloss:0.47047\n",
      "[267]\tval-logloss:0.47028\n",
      "[268]\tval-logloss:0.47047\n",
      "[269]\tval-logloss:0.47034\n",
      "[270]\tval-logloss:0.46997\n",
      "[271]\tval-logloss:0.47012\n",
      "[272]\tval-logloss:0.46995\n",
      "[273]\tval-logloss:0.46971\n",
      "[274]\tval-logloss:0.47003\n",
      "[275]\tval-logloss:0.46948\n",
      "[276]\tval-logloss:0.46892\n",
      "[277]\tval-logloss:0.46892\n",
      "[278]\tval-logloss:0.46901\n",
      "[279]\tval-logloss:0.46913\n",
      "[280]\tval-logloss:0.46870\n",
      "[281]\tval-logloss:0.46847\n",
      "[282]\tval-logloss:0.46866\n",
      "[283]\tval-logloss:0.46896\n",
      "Model saved successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\try-quant\\quantvenv\\Lib\\site-packages\\sklearn\\calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw probabilities (sample): [0.09093307 0.41279334 0.02464595 0.6985987  0.3689653  0.28824\n",
      " 0.25989565 0.35912743 0.03618515 0.48203564]\n",
      "Calibrated probabilities (sample): [0.06832298 0.17791411 0.02919708 0.32114881 0.16505441 0.12702702\n",
      " 0.14692983 0.16505441 0.02919708 0.23225152]\n",
      "\n",
      "Threshold: 0.38\n",
      "Accuracy: 0.8420\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91     11997\n",
      "           1       0.42      0.17      0.24      2095\n",
      "\n",
      "    accuracy                           0.84     14092\n",
      "   macro avg       0.64      0.56      0.57     14092\n",
      "weighted avg       0.80      0.84      0.81     14092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Main execution\n",
    "df = load_data(\"D:/try-quant/processed_data_version_7.csv\")\n",
    "df = feature_engineering(df)\n",
    "df = define_target(df)\n",
    "\n",
    "feature_cols = [col for col in df.columns if col not in ['target', 'datetime']]\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "\n",
    "print(\"Original class distribution:\", np.bincount(y))\n",
    "\n",
    "X_res, y_res = undersample_majority(X, y, target_size=60000)\n",
    "print(\"Class distribution after undersampling:\", np.bincount(y_res))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_res)\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X_scaled, y_res, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train_orig, X_val, y_train_orig, y_val = train_test_split(X_temp, y_temp, test_size=0.25, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Train (before SMOTE) class distribution:\", np.bincount(y_train_orig))\n",
    "print(\"Validation class distribution:\", np.bincount(y_val))\n",
    "print(\"Test class distribution:\", np.bincount(y_test))\n",
    "\n",
    "smote = SMOTE(sampling_strategy={0: 36044, 1: 28000}, random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train_orig, y_train_orig)\n",
    "\n",
    "print(\"Train class distribution (after SMOTE):\", np.bincount(y_train))\n",
    "\n",
    "best_params = optimize_xgboost(X_train, y_train, X_val, y_val)\n",
    "model = train_model(X_train, y_train, X_val, y_val, best_params)\n",
    "evaluate_model(model, best_params, X_train, y_train, X_val, y_val, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
